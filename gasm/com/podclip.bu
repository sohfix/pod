#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
clipex — cut podcast/music clips; batch, concat, crossfade, gaps, loudness, and work in named spaces.

Quick examples:
  # Single clip with safe edges (millisecond precision supported)
  clipex -i INPUT.mp3 --start 3:50.250 --end 4:50.750 --name intro --edge-fade 0.02

  # Play clip after cutting
  clipex -i INPUT.mp3 --start 10 --end 20 --name sample --play

  # Batch (CSV) → individual files
  clipex -i INPUT.mp3 -f clips.csv

  # Batch → one highlight reel with 1.0s crossfades + loudness target
  clipex -i INPUT.mp3 -f clips.csv --concat --name highlight_reel --xfade 1.0 --loudnorm -16

  # Concat existing clips (skip cutting)
  clipex --parts clipA.mp3 clipB.mp3 clipC.mp3 --concat --name mega_mix --xfade 0.75 --loudnorm -16

  # Multi-input CSV with per-row source
  clipex -i ep1.mp3 -i ep2.mp3 -i ep3.mp3 -f clips.csv --concat --name supercut --xfade 1.0

  # Set a default output directory
  clipex set --output-default ~/media/audio/clipex

  # Work in spaces (namespaces)
  clipex space create mypod
  clipex space set --in ~/Podcasts/mypod --out ~/Clips/mypod
  clipex space start mypod
  clipex -i episode01.mp3 -f cuts.csv --concat --name ep01_supercut   # uses space I/O by default
  clipex space list
  clipex space end
"""

import argparse
import csv
import json
import os
import platform
import re
import shutil
import subprocess
import sys
import tempfile
import textwrap
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

APP_NAME   = "clipex"
APP_DESC   = "Cut podcast/music clips from MP3s; batch, concat, crossfade, gaps, loudness, and spaces.\n Run clipex podcast --help for downloading media."
APP_VER    = "1.7.1"

CONFIG_DIR = Path(os.environ.get("XDG_CONFIG_HOME", Path.home() / "apps/.app-data/.config-files")) / APP_NAME
CONFIG_PATH= CONFIG_DIR / "config.json"

# Globals toggled by CLI
VERBOSE   = False
OVERWRITE = False

# ---------- Help & UX ----------

class _SmartFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawTextHelpFormatter):
    """Shows defaults + preserves newlines/indent."""
    pass

def _dedent(s: str) -> str:
    return textwrap.dedent(s).strip("\n")

EXAMPLES_TEXT = _dedent(f"""
General:
  {APP_NAME} --help
  {APP_NAME} --version
  {APP_NAME} --examples
  {APP_NAME} help run
  {APP_NAME} help set
  {APP_NAME} help space

Single clip:
  # Times accept M:SS(.ms), H:MM:SS(.ms), or seconds(.ms)
  {APP_NAME} -i INPUT.mp3 --start "3:50.250" --end "4:50.750" --name intro
  {APP_NAME} -i INPUT.mp3 --start 235.1 --end 300.9 --name funny_bit --edge-fade 0.02
  {APP_NAME} -i INPUT.mp3 --start 3:55 --end 4:55.500 --name teaser --mono --cbr 192k --play

Batch (CSV format):
  # clips.csv
  #default, Highlights from the episode
  start,end,name,description
  3:55,4:55.250,hot_take,#default
  2:35.100,3:33.900,funny_bit,Hosts crack up
  420.0,505.5,,

  {APP_NAME} -i INPUT.mp3 -f clips.csv

Multi-input CSV (per-row 'source' optional):
  # clips.csv
  #default, Supercut across three pods
  start,end,name,description,source
  3:55,4:55.250,ep1_hot_take,#default,ep1.mp3
  2:35.100,3:33.900,ep2_funny,Hosts crack up,ep2.mp3
  420.0,505.5,ep3_reflection,,ep3.mp3

  {APP_NAME} -i ep1.mp3 -i ep2.mp3 -i ep3.mp3 -f clips.csv --concat --name supercut --xfade 1.0

Concat (one highlight reel):
  {APP_NAME} -i INPUT.mp3 -f clips.csv --concat --name highlight_reel

Concat + crossfade + loudness:
  {APP_NAME} -i INPUT.mp3 -f clips.csv --concat --name highlight_reel --xfade 1.0 --loudnorm -16

Concat + gaps (no crossfade):
  {APP_NAME} -i INPUT.mp3 -f clips.csv --concat --gap 0.25 --name teaser_mix

Keep the cut parts too:
  {APP_NAME} -i INPUT.mp3 -f clips.csv --concat --xfade 1.0 --keep-clips

Concat existing clips (skip cutting):
  {APP_NAME} --parts clipA.mp3 clipB.mp3 clipC.mp3 --concat --name mega_mix --xfade 0.75 --loudnorm -16

Set defaults:
  {APP_NAME} set --output-default ~/ClipexClips

Spaces:
  {APP_NAME} space create myshow
  {APP_NAME} space set --in ~/Podcasts/myshow --out ~/Clips/myshow
  {APP_NAME} space start myshow
  {APP_NAME} -i ep001.mp3 --start 30 --end 90 --name opener
  {APP_NAME} space list
  {APP_NAME} space end
""")

def print_examples_and_exit():
    print(EXAMPLES_TEXT)
    sys.exit(0)

def print_topic_help_and_exit(parser: argparse.ArgumentParser, topic: Optional[str]):
    # topic can be "run" or "set" or "space" or None
    if not topic or topic.lower() not in {"run", "set", "space"}:
        parser.print_help()
        sys.exit(0)
    # find the subparser by name and print its help
    subparsers_action = next(a for a in parser._actions if isinstance(a, argparse._SubParsersAction))
    sp = subparsers_action.choices[topic.lower()]
    sp.print_help()
    sys.exit(0)

# ---------- Config & Spaces ----------

def load_config() -> dict:
    if CONFIG_PATH.exists():
        try:
            return json.loads(CONFIG_PATH.read_text())
        except Exception:
            pass
    return {}

def save_config(cfg: dict) -> None:
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    CONFIG_PATH.write_text(json.dumps(cfg, indent=2))

def get_spaces(cfg: Dict[str, Any]) -> Dict[str, Any]:
    return cfg.setdefault("spaces", {})

def get_current_space(cfg: Dict[str, Any]) -> Optional[str]:
    return cfg.get("current_space")

def set_current_space(cfg: Dict[str, Any], name: Optional[str]) -> None:
    if name is None:
        cfg.pop("current_space", None)
    else:
        cfg["current_space"] = name
    save_config(cfg)

def ensure_outdir(path: Optional[str], cfg: dict) -> Path:
    """
    Resolves the output directory in priority:
      1) --output
      2) current space's 'out'
      3) configured default (clipex set --output-default DIR)
      4) current directory
    Creates the directory if missing.
    """
    out = None
    if path:
        out = Path(path).expanduser()
    else:
        cur = get_current_space(cfg)
        if cur:
            out = Path(get_spaces(cfg).get(cur, {}).get("out", "")) if get_spaces(cfg).get(cur, {}).get("out") else None
        if not out:
            out = Path(cfg.get("output_default", "."))

    out.mkdir(parents=True, exist_ok=True)
    return out

def resolve_input_path(user_path: str, cfg: dict) -> Path:
    """
    Resolve an input path, trying as given; if not found and current space has 'in', try under that.
    """
    p = Path(user_path).expanduser()
    if p.exists():
        return p
    cur = get_current_space(cfg)
    if cur:
        in_base = get_spaces(cfg).get(cur, {}).get("in")
        if in_base:
            p2 = Path(in_base).expanduser() / user_path
            if p2.exists():
                return p2
    return p  # will be validated by callers

# ---------- ffmpeg helpers ----------

def ffmpeg_base_args() -> List[str]:
    args = ["-hide_banner"]
    args += ["-loglevel", "info" if VERBOSE else "error"]
    if OVERWRITE:
        args += ["-y"]
    return args

def run_subprocess(cmd: List[str]) -> int:
    if VERBOSE:
        print(">>", " ".join(cmd))
    return subprocess.run(cmd).returncode

# ---------- Utilities ----------

# Updated to support milliseconds:
# - M:SS(.ms)
# - H:MM:SS(.ms)
# - seconds(.ms)
TIME_PATTERNS = [
    re.compile(r"^(?P<m>\d+):(?P<s>[0-5]?\d(?:\.\d+)?)$"),                 # M:SS(.ms)
    re.compile(r"^(?P<h>\d+):(?P<m>[0-5]?\d):(?P<s>[0-5]?\d(?:\.\d+)?)$"), # H:MM:SS(.ms)
    re.compile(r"^(?P<s>\d+(?:\.\d+)?)$"),                                 # seconds(.ms)
]

def parse_time(ts: str) -> float:
    """
    Parse time strings with optional milliseconds.
    Accepts: M:SS(.ms) | H:MM:SS(.ms) | seconds(.ms)
    """
    s = ts.strip()
    for pat in TIME_PATTERNS:
        m = pat.match(s)
        if m:
            h = float(m.groupdict().get("h") or 0)
            m_ = float(m.groupdict().get("m") or 0)
            s_ = float(m.groupdict().get("s") or 0)
            return h * 3600.0 + m_ * 60.0 + s_
    raise ValueError(f"Invalid time format: {ts!r}  (use M:SS(.ms), H:MM:SS(.ms), or seconds(.ms))")

def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/:*?"<>|]+', "_", name).strip()

def choose_encode_args(cbr: Optional[str], mono: bool) -> List[str]:
    """
    Audio encode args for final renders (or parts when re-encoding).
    Default: VBR -q:a 2. If cbr like '192k', use -b:a 192k. --mono downmixes.
    """
    args = ["-acodec", "libmp3lame"]
    if cbr:
        args += ["-b:a", str(cbr)]
    else:
        args += ["-q:a", "2"]  # good VBR for podcasts
    if mono:
        args += ["-ac", "1"]
    return args

# ---------- Player ----------

def find_player() -> Optional[List[str]]:
    """
    Returns a command list for a suitable audio player (blocking preferred).
    Tries: ffplay -autoexit, afplay (macOS), mpg123, cvlc, play (sox), powershell WMPlayer fallback.
    """
    if shutil.which("ffplay"):
        return ["ffplay", *ffmpeg_base_args(), "-autoexit"]
    if platform.system() == "Darwin" and shutil.which("afplay"):
        return ["afplay"]
    if shutil.which("mpg123"):
        return ["mpg123", "-q"]
    if shutil.which("cvlc"):
        return ["cvlc", "--play-and-exit", "--quiet"]
    if shutil.which("play"):  # from sox
        return ["play", "-q"]
    # Windows basic fallback using start (non-blocking) – still acceptable
    if platform.system() == "Windows":
        return ["cmd", "/c", "start", ""]
    return None

def play_file(path: Path) -> None:
    cmd = find_player()
    if not cmd:
        print(f"note: no suitable audio player found to play {path}", file=sys.stderr)
        return
    run_subprocess(cmd + [str(path)])

# ---------- Cutting & parsing ----------

def run_ffmpeg_cut(
    input_mp3: Path,
    start_s: float,
    end_s: float,
    out_path: Path,
    title: Optional[str] = None,
    comment: Optional[str] = None,
    copy: bool = False,
    edge_fade: float = 0.0,
    cbr: Optional[str] = None,
) -> None:
    """
    Cut a segment from input. If edge_fade>0, apply tiny fades to prevent clicks (re-encode).
    If copy=True and edge_fade>0 or cbr set, copy is ignored (we re-encode).
    """
    if end_s <= start_s:
        raise ValueError("end must be greater than start")
    duration = end_s - start_s

    meta = []
    if title:   meta += ["-metadata", f"title={title}"]
    if comment: meta += ["-metadata", f"comment={comment}"]

    cmd = [
        "ffmpeg", *ffmpeg_base_args(),
        "-ss", f"{start_s:.3f}", "-t", f"{duration:.3f}",
        "-i", str(input_mp3),
    ]

    if copy and edge_fade <= 0 and not cbr:
        codec_args = ["-c", "copy"]
        filter_args: List[str] = []
    else:
        codec_args = choose_encode_args(cbr=cbr, mono=False)
        filter_chain = []
        if edge_fade > 0:
            st_out = max(0.0, duration - edge_fade)
            filter_chain.append(f"afade=t=in:st=0:d={edge_fade}")
            filter_chain.append(f"afade=t=out:st={st_out:.3f}:d={edge_fade}")
        filter_args = ["-af", ",".join(filter_chain)] if filter_chain else []
        if copy and (edge_fade > 0 or cbr):
            print(f"note: re-encoding {out_path.name} (copy disabled by --edge-fade/--cbr)")

    cmd += filter_args + codec_args + meta + [str(out_path)]
    if run_subprocess(cmd) != 0:
        raise RuntimeError(f"ffmpeg failed for {out_path.name}")

def sniff_has_header(row: List[str]) -> bool:
    joined = ",".join(c.strip().lower() for c in row)
    return "start" in joined and "end" in joined

def parse_batch_file(path: Path) -> Tuple[str, List[dict]]:
    """
    CSV format (now allows optional 'source' per row):
      #default, Your default description (optional; first such line wins)
      start,end,name,description[,source]
      3:55.000,4:55.250,hot_take,#default,ep1.mp3
      2:35.100,3:33.900,funny_bit,Hosts crack up,ep2.mp3
      420.0,505.5,,,
    """
    default_desc = ""
    clips: List[dict] = []
    with path.open(newline="", encoding="utf-8") as f:
        reader = csv.reader(f)
        first_row = None
        for raw in reader:
            if not raw or (len(raw) == 1 and not raw[0].strip()):
                continue
            if raw[0].strip().startswith("#default"):
                if len(raw) >= 2:
                    default_desc = ",".join(raw[1:]).strip()
                continue
            if raw[0].strip().startswith("#"):
                continue
            if first_row is None:
                first_row = raw
                if sniff_has_header(first_row):
                    first_row = None
                    continue
            row = raw if first_row is None else first_row
            first_row = None

            # normalize to up to 5 cols: start,end,name,description,source?
            row = (row + ["", "", "", "", ""])[:5]
            start, end, name, desc, source = [c.strip() for c in row[:5]]
            if not start or not end:
                raise ValueError(f"Batch row missing start/end: {row}")
            start_s = parse_time(start); end_s = parse_time(end)
            nm = sanitize_filename(name) if name else ""
            description = default_desc if (desc == "#default" or desc == "") else desc
            clips.append({
                "start_s": start_s,
                "end_s": end_s,
                "name": nm,
                "description": description,
                "source": source,
            })
    return default_desc, clips

def derive_outfile(base_dir: Path, base_name: Optional[str], idx: int) -> Path:
    stem = sanitize_filename(base_name) if base_name else f"clip_{idx:02d}"
    return base_dir / f"{stem}.mp3"

# ---------- Concat helpers ----------

def make_silence_mp3(path: Path, duration: float, cbr: Optional[str], mono: bool) -> None:
    """Create a silent MP3 of given duration (seconds) using anullsrc."""
    if duration <= 0:
        return
    cmd = [
        "ffmpeg", *ffmpeg_base_args(),
        "-f", "lavfi", "-t", f"{duration:.3f}",
        "-i", "anullsrc=channel_layout=stereo:sample_rate=44100",
        *choose_encode_args(cbr=cbr, mono=mono),
        str(path)
    ]
    if run_subprocess(cmd) != 0:
        raise RuntimeError("failed to generate silence clip")

def build_xfade_filter(n_parts: int, xfade: float, shape: str,
                       add_limiter: bool, loudnorm_i: Optional[float]) -> Tuple[str, str]:
    """
    Build a filter_complex acrossfade chain.
    Returns (filter_complex, final_label).
    """
    if n_parts == 1:
        return "", "0:a"

    shape = (shape or "tri").lower()
    if shape not in {"tri", "exp", "log", "qsin"}:
        shape = "tri"

    filters = []
    prev = "0:a"
    for i in range(1, n_parts):
        cur = f"{i}:a"
        out = f"af{i}"
        filters.append(f"[{prev}][{cur}]acrossfade=d={xfade}:c1={shape}:c2={shape}[{out}]")
        prev = out

    final_label = prev
    if add_limiter:
        filters.append(f"[{final_label}]alimiter=limit=0.90[lim]")
        final_label = "lim"
    if loudnorm_i is not None:
        filters.append(f"[{final_label}]loudnorm=I={loudnorm_i}:TP=-1.5:LRA=11[ln]")
        final_label = "ln"

    return ";".join(filters), final_label

def ffmpeg_concat_list(list_file: Path, out_path: Path, copy: bool,
                       title: Optional[str], comment: Optional[str],
                       filter_a: Optional[str], cbr: Optional[str], mono: bool) -> None:
    """
    Concat demuxer path. If filter_a provided or mono/cbr requested, we re-encode; else stream copy.
    """
    meta = []
    if title:   meta += ["-metadata", f"title={title}"]
    if comment: meta += ["-metadata", f"comment={comment}"]

    cmd = ["ffmpeg", *ffmpeg_base_args(),
           "-f", "concat", "-safe", "0", "-i", str(list_file)]

    must_encode = bool(filter_a) or mono or not copy or bool(cbr)
    if must_encode:
        if filter_a:
            cmd += ["-filter:a", filter_a]
        cmd += choose_encode_args(cbr=cbr, mono=mono)
    else:
        cmd += ["-c", "copy"]

    cmd += meta + [str(out_path)]
    if run_subprocess(cmd) != 0:
        raise RuntimeError(f"ffmpeg concat failed for {out_path.name}")

def ffmpeg_concat_xfade(parts: List[Path], out_path: Path, xfade: float, shape: str,
                        title: Optional[str], comment: Optional[str],
                        cbr: Optional[str], mono: bool,
                        add_limiter: bool, loudnorm_i: Optional[float]) -> None:
    """
    Multi-input, filter_complex acrossfade with optional limiter/loudnorm.
    Always re-encodes the final output.
    """
    cmd = ["ffmpeg", *ffmpeg_base_args()]
    for p in parts:
        cmd += ["-i", str(p)]

    filter_complex, final_label = build_xfade_filter(
        n_parts=len(parts), xfade=xfade, shape=shape,
        add_limiter=add_limiter, loudnorm_i=loudnorm_i
    )

    if filter_complex:
        cmd += ["-filter_complex", filter_complex, "-map", f"[{final_label}]"]
    else:
        cmd += ["-map", "0:a"]

    cmd += choose_encode_args(cbr=cbr, mono=mono)
    if title:   cmd += ["-metadata", f"title={title}"]
    if comment: cmd += ["-metadata", f"comment={comment}"]
    cmd += [str(out_path)]
    if run_subprocess(cmd) != 0:
        raise RuntimeError(f"ffmpeg xfade failed for {out_path.name}")

# ---------- Commands ----------

def cmd_set(args):
    """Implements `clipex set`."""
    cfg = load_config()
    if args.output_default:
        cfg["output_default"] = str(Path(args.output_default).expanduser())
        save_config(cfg)
        print(f"Set output default to: {cfg['output_default']}")

def _validate_inputs_exist(inputs: List[Path]):
    for p in inputs:
        if not p.exists():
            print(f"error: input file not found: {p}", file=sys.stderr); sys.exit(2)

def cmd_run(args):
    """Implements `clipex run` (default)."""
    cfg = load_config()
    if args.examples:
        print_examples_and_exit()

    # guard: incompatible combos
    if args.parts and (args.file or args.start or args.end):
        print("error: --parts cannot be combined with --start/--end or -f/--file", file=sys.stderr); sys.exit(2)

    # resolve inputs list (may be None, 1+, or unused when every CSV row has a source)
    inputs: List[Path] = []
    if args.input:
        for i in args.input:
            p = resolve_input_path(i, cfg)
            if not p.exists():
                print(f"error: input file not found: {p}", file=sys.stderr); sys.exit(2)
            inputs.append(p)

    outdir = ensure_outdir(args.output, cfg)

    # SINGLE CLIP (no concat)
    if args.start and args.end and not args.concat and not args.parts and not args.file:
        if len(inputs) != 1:
            print("error: single-clip mode requires exactly one -i/--input", file=sys.stderr); sys.exit(2)
        input_mp3 = inputs[0]
        start_s = parse_time(args.start); end_s = parse_time(args.end)
        name = sanitize_filename(args.name) if args.name else f"{input_mp3.stem}_{int(start_s)}-{int(end_s)}"
        out_path = outdir / f"{name}.mp3"
        # stream copy only if no re-encode features requested
        copy = args.copy and args.edge_fade <= 0 and not args.mono and not args.cbr
        run_ffmpeg_cut(
            input_mp3, start_s, end_s, out_path,
            title=args.name or name,
            comment=args.info,
            copy=copy,
            edge_fade=max(0.0, args.edge_fade),
            cbr=args.cbr
        )
        if args.mono and copy:
            print("note: --mono requires re-encode; ignoring --copy for single clip")
        print(f"wrote {out_path}")
        if args.play:
            play_file(out_path)
        return

    # PARTS-ONLY CONCAT: build a supercut from existing clip files
    if args.parts:
        parts = []
        for mp in args.parts:
            p = resolve_input_path(mp, cfg)
            if not p.exists():
                print(f"error: part not found: {p}", file=sys.stderr); sys.exit(2)
            parts.append(p)

        if not args.concat:
            # if user forgot --concat, assume they wanted one file
            args.concat = True

        merged_name = sanitize_filename(args.name) if args.name else "supercut"
        merged_out = outdir / f"{merged_name}.mp3"

        if args.xfade > 0 and len(parts) >= 2:
            ffmpeg_concat_xfade(
                parts=parts,
                out_path=merged_out,
                xfade=args.xfade,
                shape=args.xfade_shape,
                title=merged_name,
                comment=args.info,
                cbr=args.cbr,
                mono=args.mono,
                add_limiter=True,
                loudnorm_i=(args.loudnorm if args.loudnorm is not None else None)
            )
            print(f"wrote {merged_out}")
        else:
            # optionally insert gaps
            tmp_dir = Path(tempfile.mkdtemp(prefix="clipex_", dir=str(outdir)))
            try:
                if args.gap > 0 and len(parts) >= 2:
                    interleaved = []
                    for idx, p in enumerate(parts):
                        interleaved.append(p)
                        if idx < len(parts) - 1:
                            sfile = tmp_dir / f"silence_{idx:02d}.mp3"
                            make_silence_mp3(sfile, args.gap, cbr=args.cbr, mono=False)
                            interleaved.append(sfile)
                    parts = interleaved

                list_file = tmp_dir / "concat.txt"
                with list_file.open("w", encoding="utf-8") as lf:
                    for p in parts:
                        lf.write(f"file '{p.as_posix()}'\n")

                # Post-filter chain for final (limiter always on; loudnorm optional)
                filter_chain = ["alimiter=limit=0.90"]
                if args.loudnorm is not None:
                    filter_chain.append(f"loudnorm=I={args.loudnorm}:TP=-1.5:LRA=11")
                filter_a = ",".join(filter_chain) if filter_chain else None

                ffmpeg_concat_list(
                    list_file=list_file,
                    out_path=merged_out,
                    copy=args.copy and not filter_a and not args.mono and not args.cbr,
                    title=merged_name,
                    comment=args.info,
                    filter_a=filter_a,
                    cbr=args.cbr,
                    mono=args.mono
                )
                print(f"wrote {merged_out}")
            finally:
                for p in tmp_dir.glob("*"):
                    try: p.unlink()
                    except: pass
                try: tmp_dir.rmdir()
                except: pass
        if args.play:
            play_file(merged_out)
        return

    # BATCH / CONCAT
    if args.file:
        batch_path = resolve_input_path(args.file, cfg)
        if not batch_path.exists():
            print(f"error: batch file not found: {batch_path}", file=sys.stderr); sys.exit(2)

        default_desc, clips = parse_batch_file(batch_path)
        if not clips:
            print("no clips found in batch file", file=sys.stderr); sys.exit(2)

        # Prepare cut parts
        tmp_dir = None
        if args.concat:
            tmp_dir = Path(tempfile.mkdtemp(prefix="clipex_", dir=str(outdir)))
        try:
            parts: List[Path] = []
            for i, c in enumerate(clips, 1):
                # resolve source for this row:
                src_text = c.get("source") or ""
                if src_text:
                    src_path = resolve_input_path(src_text, cfg)
                    if not src_path.exists():
                        print(f"error: source not found for row {i}: {src_path}", file=sys.stderr); sys.exit(2)
                    input_mp3 = src_path
                else:
                    if not inputs:
                        print("error: no input provided for row without 'source' (use -i/--input or add 'source' in CSV)", file=sys.stderr); sys.exit(2)
                    input_mp3 = inputs[0]

                if args.concat:
                    part_path = (tmp_dir / f"part_{i:02d}.mp3")
                else:
                    part_path = derive_outfile(outdir, c.get("name"), i)

                part_copy = args.copy and args.edge_fade <= 0 and not args.cbr
                run_ffmpeg_cut(
                    input_mp3, c["start_s"], c["end_s"], part_path,
                    title=part_path.stem,
                    comment=c.get("description") or args.info or default_desc,
                    copy=part_copy,
                    edge_fade=max(0.0, args.edge_fade),
                    cbr=args.cbr
                )
                parts.append(part_path)
                if not args.concat:
                    print(f"wrote {part_path}")

            if not args.concat:
                if args.play and parts:
                    print("note: --play with batch (no --concat) will play only the last written clip.")
                    play_file(parts[-1])
                return

            # CONCAT PATH
            merged_name = sanitize_filename(args.name) if args.name else f"{Path(inputs[0]).stem if inputs else 'highlights'}_highlights"
            merged_out = outdir / f"{merged_name}.mp3"

            # If both xfade and gap provided, prefer xfade
            if args.gap > 0 and args.xfade > 0:
                print("note: both --xfade and --gap provided; using --xfade and ignoring --gap")

            if args.xfade > 0 and len(parts) >= 2:
                ffmpeg_concat_xfade(
                    parts=parts,
                    out_path=merged_out,
                    xfade=args.xfade,
                    shape=args.xfade_shape,
                    title=merged_name,
                    comment=args.info or default_desc,
                    cbr=args.cbr,
                    mono=args.mono,
                    add_limiter=True,
                    loudnorm_i=(args.loudnorm if args.loudnorm is not None else None)
                )
                print(f"wrote {merged_out}")
            else:
                # Plain concat via demuxer; optionally insert gaps (silence files)
                if args.gap > 0 and len(parts) >= 2:
                    interleaved: List[Path] = []
                    for idx, p in enumerate(parts):
                        interleaved.append(p)
                        if idx < len(parts) - 1:
                            sfile = tmp_dir / f"silence_{idx:02d}.mp3"
                            make_silence_mp3(sfile, args.gap, cbr=args.cbr, mono=False)
                            interleaved.append(sfile)
                    parts = interleaved

                list_file = tmp_dir / "concat.txt"
                with list_file.open("w", encoding="utf-8") as lf:
                    for p in parts:
                        lf.write(f"file '{p.as_posix()}'\n")

                # Post-filter chain for final (limiter always on; loudnorm optional)
                filter_chain = ["alimiter=limit=0.90"]
                if args.loudnorm is not None:
                    filter_chain.append(f"loudnorm=I={args.loudnorm}:TP=-1.5:LRA=11")
                filter_a = ",".join(filter_chain) if filter_chain else None

                ffmpeg_concat_list(
                    list_file=list_file,
                    out_path=merged_out,
                    copy=args.copy and not filter_a and not args.mono and not args.cbr,
                    title=merged_name,
                    comment=args.info or default_desc,
                    filter_a=filter_a,
                    cbr=args.cbr,
                    mono=args.mono
                )
                print(f"wrote {merged_out}")

            if args.keep_clips:
                clips_dir = outdir / f"{merged_name}_parts"
                clips_dir.mkdir(exist_ok=True)
                for p in tmp_dir.glob("part_*.mp3"):
                    p.replace(clips_dir / p.name)
                for p in tmp_dir.glob("silence_*.mp3"):
                    p.replace(clips_dir / p.name)
                print(f"kept parts in {clips_dir}")

            if args.play:
                play_file(merged_out)

        finally:
            if tmp_dir and (not args.keep_clips):
                for p in tmp_dir.glob("*"):
                    try: p.unlink()
                    except: pass
                try: tmp_dir.rmdir()
                except: pass
        return

    print("error: provide --start and --end for a single clip, or -f/--file for batch/concat, or --parts for existing clips.", file=sys.stderr)
    sys.exit(2)

# ---------- Spaces subcommands ----------

def cmd_space(args):
    """
    clipex space [create NAME | start NAME | end [NAME] | set (--in DIR | --out DIR | --list-io) | list | change NAME | delete NAME]
    """
    cfg = load_config()
    spaces = get_spaces(cfg)

    if args.action == "create":
        name = args.name
        if not name:
            print("error: space create requires a NAME", file=sys.stderr); sys.exit(2)
        if name in spaces:
            print(f"error: space '{name}' already exists", file=sys.stderr); sys.exit(2)
        spaces[name] = {"in": "", "out": ""}
        save_config(cfg)
        print(f"created space '{name}'")

    elif args.action in {"start", "change"}:
        name = args.name
        if not name:
            print(f"error: space {args.action} requires a NAME", file=sys.stderr); sys.exit(2)
        if name not in spaces:
            print(f"error: space '{name}' not found", file=sys.stderr); sys.exit(2)
        set_current_space(cfg, name)
        print(f"current space: {name}")

    elif args.action == "end":
        name = args.name
        cur = get_current_space(cfg)
        if name and name != cur:
            print(f"error: space '{name}' is not the current space (current is '{cur}')", file=sys.stderr); sys.exit(2)
        set_current_space(cfg, None)
        print("cleared current space")

    elif args.action == "set":
        cur = get_current_space(cfg)
        if not cur:
            print("error: no current space. Use 'clipex space start NAME' first.", file=sys.stderr); sys.exit(2)
        sp = spaces.setdefault(cur, {"in": "", "out": ""})
        changed = False
        if args.in_dir:
            sp["in"] = str(Path(args.in_dir).expanduser())
            changed = True
        if args.out_dir:
            sp["out"] = str(Path(args.out_dir).expanduser())
            changed = True
        if changed:
            save_config(cfg)
            print(f"updated space '{cur}': in={sp.get('in') or '(unset)'} out={sp.get('out') or '(unset)'}")
        if args.list_io or not changed:
            print(f"space '{cur}' I/O:")
            print(f"  in : {sp.get('in') or '(unset)'}")
            print(f"  out: {sp.get('out') or '(unset)'}")

    elif args.action == "list":
        cur = get_current_space(cfg)
        if not spaces:
            print("(no spaces)")
            return
        for name, meta in spaces.items():
            star = "*" if name == cur else " "
            print(f"{star} {name}")
            if VERBOSE:
                print(f"    in : {meta.get('in') or '(unset)'}")
                print(f"    out: {meta.get('out') or '(unset)'}")

    elif args.action == "delete":
        name = args.name
        if not name:
            print("error: space delete requires a NAME", file=sys.stderr); sys.exit(2)
        if name not in spaces:
            print(f"error: space '{name}' not found", file=sys.stderr); sys.exit(2)
        if get_current_space(cfg) == name:
            set_current_space(cfg, None)
        spaces.pop(name, None)
        save_config(cfg)
        print(f"deleted space '{name}'")

    else:
        print("error: unknown space action", file=sys.stderr); sys.exit(2)

# ---------- CLI ----------

def make_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog=APP_NAME,
        description=APP_DESC,
        formatter_class=_SmartFormatter,
        epilog=_dedent("""\
            Tips:
              • Times accept M:SS(.ms), H:MM:SS(.ms), or raw seconds(.ms).
              • Use --copy for speed when you don't need fades/filters/mono/CBR.
              • CSV '#default, text...' sets a default description for empty/#default rows.
              • With --concat, prefer --xfade for smooth transitions or --gap for separation.
              • You can pass multiple -i/--input, and/or use a per-row 'source' in CSV.
              • Or skip cutting entirely with --parts to merge existing clips.
              • Use 'clipex space' to organize input/output defaults per project.
            """),
        add_help=False  # we'll add -h/--help manually to include in all subparsers too
    )

    # Global flags
    p.add_argument("-h","--help", action="help", help="Show this help message and exit")
    p.add_argument("--version", action="version", version=f"%(prog)s {APP_VER}", help="Show version and exit")
    p.add_argument("--examples", action="store_true", help="Print practical usage recipes and exit")
    p.add_argument("-y","--yes", action="store_true", help="Overwrite output files without asking (passes -y to ffmpeg)")
    p.add_argument("-v","--verbose", action="store_true", help="Verbose output (show ffmpeg commands, list space I/O)")

    sub = p.add_subparsers(dest="cmd", metavar="COMMAND", title="Commands")

    # ---- run ----
    pr = sub.add_parser(
        "run", help="Run clipex (default)", formatter_class=_SmartFormatter, add_help=False,
        description=_dedent("""\
            Run clipex to cut clips from MP3s.
            Modes:
              • Single clip:   --start + --end (outputs one file, requires exactly one -i)
              • Batch:         -f/--file CSV (outputs many files)
              • Concat:        --concat on a batch (one highlight reel)
              • Parts concat:  --parts (skip cutting; just merge existing clips)
            """)
    )
    pr.add_argument("-h","--help", action="help", help="Show help for 'run' and exit")
    pr.add_argument("--examples", action="store_true", help="Show 'run' examples and exit")

    # IO
    pr.add_argument("-i","--input", metavar="FILE", action="append",
                    help="Input MP3 file. Can be given multiple times; rows without a 'source' column use the first. Resolved against current space's 'in' if set.")
    pr.add_argument("-o","--output", metavar="DIR", help="Output directory (falls back to current space's 'out', then global default)")

    pr.add_argument("--name", metavar="NAME", help="Output file name (single clip) or merged name with --concat")
    pr.add_argument("--info", metavar="TEXT", help="Optional description/notes for metadata")
    pr.add_argument("-f","--file", metavar="CSV",
                    help=_dedent("""\
                        Batch CSV with rows: start,end,name,description[,source]
                        Header is optional (auto-detected). Lines starting with '#default' set
                        a default description for rows that use '#default' or empty description.
                        'source' (optional per row) can reference a different input file for that clip
                        and will be resolved against the current space's 'in' if set.
                        Times support milliseconds: M:SS(.ms), H:MM:SS(.ms), or seconds(.ms)
                    """))
    pr.add_argument("--parts", nargs="+", metavar="MP3",
                    help="Existing clip files to concat (skip cutting). Incompatible with --start/--end and -f/--file.")

    # Single-clip selection
    pr.add_argument("--start", metavar="TIME", help="Start time (M:SS(.ms), H:MM:SS(.ms), or seconds(.ms))")
    pr.add_argument("--end",   metavar="TIME", help="End time (M:SS(.ms), H:MM:SS(.ms), or seconds(.ms))")

    # Performance/accuracy
    pr.add_argument("--copy", action="store_true",
                    help="Try stream copy (fast, less accurate; disabled if filters/mono/CBR are used)")

    # Concat controls
    pr.add_argument("--concat", action="store_true", help="Concatenate all batch clips into one MP3")
    pr.add_argument("--keep-clips", action="store_true", help="With --concat, keep the intermediate cut parts")

    # Audio shaping
    pr.add_argument("--xfade", type=float, default=1.0,
                    help="Crossfade seconds between clips (with --concat). Set 0 to disable.")
    pr.add_argument("--xfade-shape", choices=["tri","log","exp","qsin"], default="tri",
                    help="Crossfade curve shape")
    pr.add_argument("--gap", type=float, default=0.0,
                    help="Silence (seconds) inserted between clips when not using --xfade")
    pr.add_argument("--edge-fade", type=float, default=0.02,
                    help="Tiny fade at start/end of each cut part to prevent clicks")
    pr.add_argument("--loudnorm", type=float, default=None,
                    help="Target LUFS for final loudness (e.g., -16 for podcasts). Re-encodes final.")
    pr.add_argument("--mono", action="store_true", help="Downmix the final output to mono")
    pr.add_argument("--cbr", type=str, default=None, metavar="RATE",
                    help="Force constant bitrate for encoding (e.g., 192k). Default is VBR -q:a 2.")

    # UX niceties
    pr.add_argument("--play", action="store_true", help="Play the resulting clip/output after writing it")

    pr.set_defaults(func=cmd_run)

    # ---- set ----
    ps = sub.add_parser(
        "set", help="Set global defaults", formatter_class=_SmartFormatter, add_help=False,
        description=f"Configure persistent defaults (stored under {CONFIG_PATH})."
    )
    ps.add_argument("-h","--help", action="help", help="Show help for 'set' and exit")
    ps.add_argument("--output-default", metavar="DIR", help="Default output directory for all runs (used when space 'out' and --output are not set)")
    ps.set_defaults(func=cmd_set)

    # ---- space ----
    pspace = sub.add_parser(
        "space", help="Manage named spaces (namespaces) for I/O presets", add_help=False, formatter_class=_SmartFormatter,
        description=_dedent("""\
            Work with project 'spaces' (namespaces) that remember input/output directories.
            Actions:
              • create NAME        Create a space
              • start NAME         Make space current (also: 'change')
              • end [NAME]         Clear current space (NAME must match if provided)
              • set [--in DIR] [--out DIR] [--list-io]   Update/list current space I/O
              • list               List spaces (current one is marked with *)
              • delete NAME        Remove a space
        """)
    )
    pspace.add_argument("-h","--help", action="help", help="Show help for 'space' and exit")
    space_sub = pspace.add_subparsers(dest="action", metavar="ACTION")

    sp_create = space_sub.add_parser("create", help="Create a space", add_help=False)
    sp_create.add_argument("name", help="Space name")
    sp_create.set_defaults(func=cmd_space)

    sp_start = space_sub.add_parser("start", help="Start/use a space", add_help=False)
    sp_start.add_argument("name", help="Space name")
    sp_start.set_defaults(func=cmd_space)

    sp_change = space_sub.add_parser("change", help="Alias of start", add_help=False)
    sp_change.add_argument("name", help="Space name")
    sp_change.set_defaults(func=cmd_space)

    sp_end = space_sub.add_parser("end", help="Clear current space", add_help=False)
    sp_end.add_argument("name", nargs="?", help="(optional) Must match current space if provided")
    sp_end.set_defaults(func=cmd_space)

    sp_set = space_sub.add_parser("set", help="Set I/O for current space", add_help=False)
    sp_set.add_argument("--in", dest="in_dir", metavar="DIR", help="Default input base directory for current space")
    sp_set.add_argument("--out", dest="out_dir", metavar="DIR", help="Default output directory for current space")
    sp_set.add_argument("--list-io", action="store_true", help="List current space I/O")
    sp_set.set_defaults(func=cmd_space)

    sp_list = space_sub.add_parser("list", help="List spaces", add_help=False)
    sp_list.set_defaults(func=cmd_space)

    sp_delete = space_sub.add_parser("delete", help="Delete a space", add_help=False)
    sp_delete.add_argument("name", help="Space name")
    sp_delete.set_defaults(func=cmd_space)

    # ---- help (topic-aware) ----
    ph = sub.add_parser(
        "help", help="Show top-level help or topic help (run | set | space)",
        formatter_class=_SmartFormatter, add_help=False
    )
    ph.add_argument("-h","--help", action="help", help="Show help for 'help' and exit")
    ph.add_argument("topic", nargs="?", help="Help topic: run | set | space")
    ph.set_defaults(func=lambda a: print_topic_help_and_exit(p, a.topic))

    return p

def clipex_main(argv=None):
    global VERBOSE, OVERWRITE
    argv = sys.argv[1:] if argv is None else argv
    parser = make_parser()

    # Global --examples works from anywhere (before we potentially dispatch to subcommands)
    if "--examples" in argv and (len(argv) == 1 or argv[0] != "help"):
        print_examples_and_exit()

    # Convenience: if first token isn't a subcommand, treat as 'run'
    if argv and argv[0] not in {"run","set","help","space"}:
        argv = ["run"] + argv

    args = parser.parse_args(argv)

    # set globals
    VERBOSE = bool(getattr(args, "verbose", False))
    OVERWRITE = bool(getattr(args, "yes", False))

    # 'help' subcommand already exits; others continue
    if not hasattr(args, "func"):
        parser.print_help()
        return 2
    try:
        return args.func(args) or 0
    except (ValueError, RuntimeError) as e:
        print(f"error: {e}", file=sys.stderr)
        return 1

# (clipex entry disabled in combined build)


# ---- End: clipex core ----

# ---- Begin: podcast manager (pox) ----
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
## TODO:
#

"""
pox — CLI Podcast Manager (RSS-only, single-file)

🚦 RSS-only: This tool strictly supports RSS feeds (RSS 2.0-style).
    - Root must be <rss> (optionally namespaced) containing <channel>/<item>.
    - Atom feeds (<feed>) are rejected with an error.
    - Each episode must be discoverable under <item>, and audio is taken from:
        <enclosure url="..."> or <media:content url="..." type="audio/*">

Features:
- Init config/data dirs and PodFile (RSS feed URLs)
- Manage feeds (list/add/remove/rename, set summary)
- Refresh episodes (RSS-only) into a SQLite manifest
- Info per feed (counts, last publish)
- Search (single feed / all feeds)
- Download episodes (by latest N, all, ids) and by title
- Download queue (add/list/download/remove/reset)
- Playlists (create/add/remove/list/show/export m3u)
- Clean (clear download records per feed, optional file deletion)
- **NEW**: yt-dlp helper: download YouTube/other links to audio (default) or video
- **NEW**: Settings to control global base download dir and per-feed download dirs

Examples:
    pox init
    pox add-feed myshow https://example.com/podcast.xml
    pox list-feeds
    pox refresh --feed myshow
    pox search --feed myshow --query "episode 12"
    pox download --feed myshow --latest 3
    pox download-title --feed myshow --title "pilot"
    pox queue-add --feed myshow --title "pilot"
    pox queue-list
    pox queue-download
    pox playlist create favorites
    pox playlist export favorites --out ~/favorites.m3u

    # yt-dlp helpers
    pox set --download-yt-dir ~/media/video/yt
    pox download-yt --link https://www.youtube.com/watch?v=dQw4w9WgXcQ
    pox download-yt --link https://www.youtube.com/watch?v=dQw4w9WgXcQ --video

    # NEW settings
    pox set --download-dir ~/media/audio/podcasts
    pox set --change-feed-dir bastards --dir ~/DIRECTORY
    pox set --unset-feed-dir bastards
    pox set --show
"""

import argparse
import configparser
import contextlib
import datetime as dt
import email.utils
import os
import re
import shutil
import sqlite3
import subprocess
import sys
import textwrap
import time
import traceback
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
from typing import Dict, List, Optional, Tuple

# ---------- Paths & Constants ----------
POD_APP_NAME = "clipex pod"
DESC = "CLI Podcast Manager (RSS-only)"


def xdg_path(env_key: str, default_sub: str) -> str:
    base = os.environ.get(env_key)
    if base:
        return os.path.join(os.path.expanduser(base), POD_APP_NAME)
    home = os.path.expanduser("~")
    return os.path.join(home, default_sub, POD_APP_NAME)

# Put all config/state here (PodFile.ini + database)
POD_CONFIG_DIR = os.path.expanduser("~/apps/.app-data/.config-files/pox")
DATA_DIR = POD_CONFIG_DIR
DB_PATH = os.path.join(POD_CONFIG_DIR, "pox.db")
PODFILE_PATH = os.path.join(POD_CONFIG_DIR, "PodFile.ini")

# Default audio downloads go here (legacy default; can be overridden by setting "download_dir")
DOWNLOADS_DIR = os.path.expanduser("~/media/audio/default/pox")

# Default yt-dlp downloads dir (can be overridden by `pox set --download-yt-dir`)
DEFAULT_YT_DIR = os.path.expanduser("~/media/video/default/pox-yt")

USER_AGENT = f"{POD_APP_NAME}/1.0 (+https://example.invalid)"
HTTP_TIMEOUT = 45

SQL_PRAGMAS = [
    "PRAGMA foreign_keys = ON",
    "PRAGMA journal_mode = WAL",
    "PRAGMA synchronous = NORMAL",
]

# ---------- Utilities ----------


def eprint(*a, **k):
    print(*a, file=sys.stderr, **k)


def read_podfile() -> configparser.ConfigParser:
    ensure_minimal_dirs_only()
    cfg = configparser.ConfigParser()
    if os.path.exists(PODFILE_PATH):
        cfg.read(PODFILE_PATH)
    else:
        cfg["feeds"] = {}
        cfg["summaries"] = {}
        cfg["settings"] = {}
        with open(PODFILE_PATH, "w", encoding="utf-8") as f:
            cfg.write(f)
    if "feeds" not in cfg:
        cfg["feeds"] = {}
    if "summaries" not in cfg:
        cfg["summaries"] = {}
    if "settings" not in cfg:
        cfg["settings"] = {}
    return cfg


def write_podfile(cfg: configparser.ConfigParser):
    ensure_minimal_dirs_only()
    with open(PODFILE_PATH, "w", encoding="utf-8") as f:
        cfg.write(f)


def get_setting(key: str, default: Optional[str] = None) -> Optional[str]:
    cfg = read_podfile()
    return cfg.get("settings", key, fallback=default)


def set_setting(key: str, value: Optional[str]):
    cfg = read_podfile()
    if "settings" not in cfg:
        cfg["settings"] = {}
    if value is None:
        cfg["settings"].pop(key, None)
    else:
        cfg["settings"][key] = os.path.expanduser(value)
    write_podfile(cfg)


def ensure_minimal_dirs_only():
    # Helper to ensure config dirs exist before reading/writing PodFile
    os.makedirs(POD_CONFIG_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)


# ---- New: directory settings helpers ----

def get_base_downloads_dir() -> str:
    return os.path.expanduser(get_setting("download_dir", DOWNLOADS_DIR))

def _feed_dir_key(feed: str) -> str:
    # settings keys are flat; we store as feed_dir.<shortname>
    return f"feed_dir.{feed}"

def get_feed_download_dir(feed: str) -> str:
    override = get_setting(_feed_dir_key(feed))
    if override:
        return os.path.expanduser(override)
    return os.path.join(get_base_downloads_dir(), feed)

def set_feed_download_dir(feed: str, path: str):
    set_setting(_feed_dir_key(feed), path)

def unset_feed_download_dir(feed: str):
    set_setting(_feed_dir_key(feed), None)

def list_feed_dir_overrides() -> Dict[str, str]:
    cfg = read_podfile()
    out: Dict[str, str] = {}
    settings = cfg["settings"]
    for k, v in settings.items():
        if k.startswith("feed_dir."):
            out[k.split(".", 1)[1]] = os.path.expanduser(v)
    return out


def ensure_dirs():
    # Config/data dirs
    os.makedirs(POD_CONFIG_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)

    # Global base audio dir (may be overridden later per-feed)
    base = get_base_downloads_dir()
    os.makedirs(base, exist_ok=True)

    # Back-compat: if base changed from legacy constant, also ensure the legacy path exists
    if base != DOWNLOADS_DIR:
        os.makedirs(DOWNLOADS_DIR, exist_ok=True)

    # Per-feed overrides
    for _feed, path in list_feed_dir_overrides().items():
        with contextlib.suppress(Exception):
            os.makedirs(path, exist_ok=True)

    # YT dir
    yt_dir = get_setting("download_yt_dir", DEFAULT_YT_DIR)
    if yt_dir:
        os.makedirs(os.path.expanduser(yt_dir), exist_ok=True)


def connect_db():
    ensure_dirs()
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    for p in SQL_PRAGMAS:
        cur.execute(p)
    # Create schema if not exists
    cur.executescript(
        """
    CREATE TABLE IF NOT EXISTS feeds (
        short_name TEXT PRIMARY KEY,
        url TEXT NOT NULL,
        custom_name TEXT,
        created_at TEXT NOT NULL,
        updated_at TEXT NOT NULL
    );

    CREATE TABLE IF NOT EXISTS episodes (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        feed TEXT NOT NULL,
        guid TEXT,
        title TEXT,
        pub_date TEXT,
        enclosure_url TEXT,
        link TEXT,
        description TEXT,
        created_at TEXT NOT NULL,
        UNIQUE(feed, guid),
        FOREIGN KEY(feed) REFERENCES feeds(short_name) ON DELETE CASCADE
    );

    CREATE TABLE IF NOT EXISTS downloads (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        episode_id INTEGER NOT NULL,
        feed TEXT NOT NULL,
        file_path TEXT NOT NULL,
        status TEXT NOT NULL,
        size_bytes INTEGER,
        downloaded_at TEXT NOT NULL,
        FOREIGN KEY(episode_id) REFERENCES episodes(id) ON DELETE CASCADE
    );

    CREATE TABLE IF NOT EXISTS queue (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        episode_id INTEGER NOT NULL,
        feed TEXT NOT NULL,
        added_at TEXT NOT NULL,
        FOREIGN KEY(episode_id) REFERENCES episodes(id) ON DELETE CASCADE
    );

    CREATE TABLE IF NOT EXISTS playlists (
        name TEXT PRIMARY KEY,
        created_at TEXT NOT NULL
    );

    CREATE TABLE IF NOT EXISTS playlist_items (
        playlist_name TEXT NOT NULL,
        episode_id INTEGER NOT NULL,
        position INTEGER NOT NULL,
        added_at TEXT NOT NULL,
        PRIMARY KEY (playlist_name, episode_id),
        FOREIGN KEY(playlist_name) REFERENCES playlists(name) ON DELETE CASCADE,
        FOREIGN KEY(episode_id) REFERENCES episodes(id) ON DELETE CASCADE
    );
    """
    )
    conn.commit()
    return conn


def now_iso() -> str:
    return dt.datetime.now(dt.timezone.utc).replace(microsecond=0).isoformat()


def slugify(text: str, maxlen: int = 120) -> str:
    text = text.strip().lower()
    text = re.sub(r"[^\w\s-]", "", text)
    text = re.sub(r"[\s-]+", "-", text).strip("-")
    return text[: maxlen] if text else "episode"


def require_feed_exists(conn: sqlite3.Connection, short_name: str):
    cur = conn.execute("SELECT 1 FROM feeds WHERE short_name = ?", (short_name,))
    if not cur.fetchone():
        raise SystemExit(
            f"Feed '{short_name}' not found. Use 'add-feed' first or check 'list-feeds'."
        )


def human_size(n: Optional[int]) -> str:
    if n is None:
        return "?"
    units = ["B", "KB", "MB", "GB", "TB"]
    size = float(n)
    for u in units:
        if size < 1024 or u == units[-1]:
            return f"{size:.2f} {u}"
        size /= 1024


def parse_date(s: Optional[str]) -> Optional[str]:
    if not s:
        return None
    s = s.strip()
    try:
        dtm = email.utils.parsedate_to_datetime(s)
        if dtm is not None:
            if dtm.tzinfo is None:
                dtm = dtm.replace(tzinfo=dt.timezone.utc)
            return dtm.astimezone(dt.timezone.utc).replace(microsecond=0).isoformat()
    except Exception:
        pass
    try:
        iso = s.replace("Z", "+00:00")
        dtm = dt.datetime.fromisoformat(iso)
        if dtm.tzinfo is None:
            dtm = dtm.replace(tzinfo=dt.timezone.utc)
        return dtm.astimezone(dt.timezone.utc).replace(microsecond=0).isoformat()
    except Exception:
        return s  # fallback to original


def http_get(url: str) -> bytes:
    req = urllib.request.Request(
        url,
        headers={
            "User-Agent": USER_AGENT,
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
        },
    )
    with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
        return resp.read()


def http_stream_to_file(url: str, out_path: str) -> Tuple[int, Optional[int]]:
    req = urllib.request.Request(
        url, headers={"User-Agent": USER_AGENT, "Accept": "*/*"}
    )
    with urllib.request.urlopen(req, timeout=HTTP_TIMEOUT) as resp:
        total = resp.headers.get("Content-Length")
        total_i = int(total) if total and total.isdigit() else None
        CHUNK = 1024 * 256
        written = 0
        start = time.time()
        with open(out_path, "wb") as f:
            while True:
                chunk = resp.read(CHUNK)
                if not chunk:
                    break
                f.write(chunk)
                written += len(chunk)
                if total_i:
                    pct = written * 100 // total_i
                    speed = written / max(1e-6, (time.time() - start))
                    eprint(
                        f"\r  {pct:3d}%  {human_size(written)} / {human_size(total_i)}  {human_size(int(speed))}/s",
                        end="",
                    )
        if total_i:
            eprint("\r  100% " + " " * 40)
        return written, total_i

# ---------- RSS Parsing (RSS-only) ----------


def text_of(elem: Optional[ET.Element]) -> Optional[str]:
    if elem is None:
        return None
    return (elem.text or "").strip() or None


def find_enclosure_rss(item: ET.Element) -> Optional[str]:
    # <enclosure url="..."/>
    enc = item.find("enclosure")
    if enc is not None:
        url = enc.get("url")
        if url:
            return url.strip()
    # Fallback: media:content (namespaced)
    for e in item.findall(".//{*}content"):
        if e.get("url"):
            t = (e.get("type") or "").lower()
            if (
                not t
                or t.startswith("audio/")
                or "mpeg" in t
                or "mp3" in t
                or "aac" in t
                or "ogg" in t
                or "opus" in t
                or "m4a" in t
            ):
                return e.get("url").strip()
    return None


def parse_rss_item(it: ET.Element) -> Dict[str, Optional[str]]:
    title = text_of(it.find("title"))
    guid = text_of(it.find("guid")) or text_of(it.find("{*}id"))
    link = text_of(it.find("link"))
    desc = text_of(it.find("description")) or text_of(it.find("{*}summary"))
    pub = text_of(it.find("pubDate")) or text_of(it.find("{*}date"))
    enclosure = find_enclosure_rss(it)
    return {
        "guid": guid or (enclosure or title),
        "title": title,
        "pub_date": parse_date(pub),
        "enclosure_url": enclosure,
        "link": link,
        "description": desc,
    }


def parse_feed_rss(xml_bytes: bytes) -> List[Dict[str, Optional[str]]]:
    """
    Strictly parse RSS feeds. Reject Atom (<feed>) and unknown roots.
    Returns items with keys: guid, title, pub_date, enclosure_url, link, description.
    """
    try:
        root = ET.fromstring(xml_bytes)
    except ET.ParseError as e:
        raise SystemExit(f"Failed to parse feed XML: {e}")

    tag = root.tag.lower()
    # Accept <rss ...> (possibly namespaced) or <rdf:RDF> with channel/item, but REJECT <feed>
    if tag.endswith("feed"):
        raise SystemExit(
            "This URL appears to be an Atom feed (<feed>). RSS is required. Please supply an RSS feed URL."
        )
    if not tag.endswith("rss") and "rdf" not in tag:
        raise SystemExit(f"Unsupported feed format (root tag '{root.tag}'). RSS is required.")

    channel = root.find("channel")
    if channel is None:
        # Try wildcard search (namespaced)
        channel = root.find(".//channel")
    if channel is None:
        raise SystemExit("Invalid RSS: <channel> not found.")

    items = []
    for it in channel.findall("item"):
        items.append(parse_rss_item(it))
    if not items:
        for it in channel.findall(".//item"):
            items.append(parse_rss_item(it))

    # Keep only meaningful items
    items = [i for i in items if i.get("title") or i.get("enclosure_url") or i.get("guid")]
    if not items:
        eprint("Warning: RSS parsed but contained no <item> entries.")
    return items

# ---------- CLI Actions ----------


def cmd_init(args):
    ensure_dirs()
    cfg = read_podfile()
    write_podfile(cfg)
    with connect_db() as conn:
        pass
    print(
        f"Initialized:\n"
        f"  Config:    {PODFILE_PATH}\n"
        f"  Database:  {DB_PATH}\n"
        f"  Downloads: {get_base_downloads_dir()}\n"
        f"  YT dir:    {get_setting('download_yt_dir', DEFAULT_YT_DIR)}"
    )


def cmd_list_feeds(args):
    cfg = read_podfile()
    feeds = cfg["feeds"]
    sums = cfg["summaries"]
    if not feeds:
        print("No feeds configured. Use 'add-feed' to add one (RSS only).")
        return
    print("Configured RSS feeds:")
    for sn, url in feeds.items():
        summ = (sums.get(sn) or "").strip()
        summ = f" — {summ[:80]}{'…' if len(summ) > 80 else ''}" if summ else ""
        print(f"  {sn}: {url}{summ}")


def cmd_add_feed(args):
    short = args.short_name
    url = args.url
    cfg = read_podfile()
    if short in cfg["feeds"]:
        raise SystemExit(f"Feed short name '{short}' already exists.")
    cfg["feeds"][short] = url
    write_podfile(cfg)
    with connect_db() as conn:
        now = now_iso()
        conn.execute(
            "INSERT OR REPLACE INTO feeds(short_name, url, custom_name, created_at, updated_at) VALUES (?, ?, ?, ?, ?)",
            (short, url, None, now, now),
        )
        conn.commit()
    print(f"Added RSS feed '{short}': {url}")
    print("Note: Feeds are validated as RSS during 'refresh'.")


def cmd_remove_feed(args):
    short = args.short_name
    purge = args.purge_episodes
    delete_files = args.delete_files
    cfg = read_podfile()
    if short not in cfg["feeds"]:
        raise SystemExit(f"Feed '{short}' not found.")
    url = cfg["feeds"].pop(short)
    if cfg.has_section("summaries") and cfg["summaries"].get(short):
        cfg["summaries"].pop(short, None)
    write_podfile(cfg)
    feed_dir = get_feed_download_dir(short)
    with connect_db() as conn:
        require_feed_exists(conn, short)
        if purge:
            conn.execute("DELETE FROM episodes WHERE feed = ?", (short,))
            conn.execute("DELETE FROM downloads WHERE feed = ?", (short,))
            conn.execute("DELETE FROM queue WHERE feed = ?", (short,))
        conn.execute("DELETE FROM feeds WHERE short_name = ?", (short,))
        conn.commit()
    if delete_files and os.path.isdir(feed_dir):
        shutil.rmtree(feed_dir, ignore_errors=True)
    print(
        f"Removed feed '{short}' ({url}).{' Purged episodes.' if purge else ''}{' Deleted files.' if delete_files else ''}"
    )


def cmd_rename_feed(args):
    old = args.old_short
    new = args.new_short
    if old == new:
        raise SystemExit("Old and new short names are the same.")
    cfg = read_podfile()
    if old not in cfg["feeds"]:
        raise SystemExit(f"Feed '{old}' not found.")
    if new in cfg["feeds"]:
        raise SystemExit(f"Target short name '{new}' already exists.")
    url = cfg["feeds"].pop(old)
    cfg["feeds"][new] = url
    if cfg.has_section("summaries"):
        s = cfg["summaries"].pop(old, None)
        if s is not None:
            cfg["summaries"][new] = s
    write_podfile(cfg)
    with connect_db() as conn:
        require_feed_exists(conn, old)
        conn.execute(
            "UPDATE feeds SET short_name = ?, updated_at = ? WHERE short_name = ?",
            (new, now_iso(), old),
        )
        conn.execute("UPDATE episodes SET feed = ? WHERE feed = ?", (new, old))
        conn.execute("UPDATE downloads SET feed = ? WHERE feed = ?", (new, old))
        conn.execute("UPDATE queue SET feed = ? WHERE feed = ?", (new, old))
        conn.commit()

    # Move per-feed override key if present (settings only)
    old_key = _feed_dir_key(old)
    new_key = _feed_dir_key(new)
    cfg2 = read_podfile()
    if cfg2["settings"].get(old_key) is not None:
        val = cfg2["settings"].pop(old_key)
        cfg2["settings"][new_key] = val
        write_podfile(cfg2)

    # Filesystem move logic:
    # If there's NO per-feed override, move base/<old> -> base/<new>.
    # If override exists, skip filesystem rename (paths are explicit).
    has_override = read_podfile()["settings"].get(new_key) is not None
    if not has_override:
        old_dir = os.path.join(get_base_downloads_dir(), old)
        new_dir = os.path.join(get_base_downloads_dir(), new)
        if os.path.isdir(old_dir):
            os.makedirs(os.path.dirname(new_dir), exist_ok=True)
            os.replace(old_dir, new_dir)

    print(f"Renamed feed '{old}' → '{new}'")


def cmd_set_summary(args):
    short = args.short_name
    summ = args.summary
    cfg = read_podfile()
    if short not in cfg["feeds"]:
        raise SystemExit(f"Feed '{short}' not found.")
    cfg["summaries"][short] = summ
    write_podfile(cfg)
    print(f"Set summary for '{short}'.")


def cmd_refresh(args):
    cfg = read_podfile()
    if args.all:
        targets = list(cfg["feeds"].keys())
        if not targets:
            raise SystemExit("No feeds configured.")
    else:
        if not args.feed:
            raise SystemExit("Use --feed FEED or --all.")
        if args.feed not in cfg["feeds"]:
            raise SystemExit(f"Feed '{args.feed}' not found.")
        targets = [args.feed]

    with connect_db() as conn:
        for short in targets:
            url = cfg["feeds"][short]
            print(f"[{short}] Fetching RSS: {url}")
            try:
                xml = http_get(url)
            except Exception as e:
                eprint(f"  Error fetching feed: {e}")
                continue
            try:
                items = parse_feed_rss(xml)  # RSS-only enforcement
            except SystemExit as se:
                eprint(f"  Validation failed: {se}")
                continue
            print(f"  Parsed {len(items)} RSS item(s)")
            added = 0
            now = now_iso()
            for it in items:
                try:
                    conn.execute(
                        """
                        INSERT OR IGNORE INTO episodes(feed, guid, title, pub_date, enclosure_url, link, description, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                        (
                            short,
                            it.get("guid"),
                            it.get("title"),
                            it.get("pub_date"),
                            it.get("enclosure_url"),
                            it.get("link"),
                            it.get("description"),
                            now,
                        ),
                    )
                    if conn.total_changes > 0:
                        added += 1
                except sqlite3.IntegrityError:
                    pass
            conn.execute(
                "UPDATE feeds SET updated_at = ? WHERE short_name = ?",
                (now_iso(), short),
            )
            conn.commit()
            print(f"  Added {added} new episode records")


def cmd_info(args):
    short = args.short_name
    cfg = read_podfile()
    if short not in cfg["feeds"]:
        raise SystemExit(f"Feed '{short}' not found.")
    url = cfg["feeds"][short]
    summary = cfg["summaries"].get(short, "")
    with connect_db() as conn:
        c1 = conn.execute(
            "SELECT COUNT(*) AS c FROM episodes WHERE feed = ?", (short,)
        ).fetchone()["c"]
        c2 = conn.execute(
            "SELECT COUNT(*) AS c FROM downloads WHERE feed = ? AND status = 'completed'",
            (short,),
        ).fetchone()["c"]
        last_pub = conn.execute(
            "SELECT pub_date FROM episodes WHERE feed = ? ORDER BY pub_date DESC NULLS LAST LIMIT 1",
            (short,),
        ).fetchone()
        last_pub_s = last_pub["pub_date"] if last_pub else "N/A"
    print(f"Feed:        {short}")
    print(f"URL:         {url}")
    print(f"Format:      RSS (enforced)")
    print(f"Download dir:{' ' if True else ''}{get_feed_download_dir(short)}")
    print(f"Episodes:    {c1}")
    print(f"Downloaded:  {c2}")
    print(f"Last publish:{' ' if last_pub_s!='N/A' else ''}{last_pub_s}")
    if summary:
        print("\nSummary:")
        print(textwrap.fill(summary, width=88))


def cmd_clean(args):
    short = args.short_name
    delete_files = args.delete_files
    with connect_db() as conn:
        require_feed_exists(conn, short)
        conn.execute("DELETE FROM downloads WHERE feed = ?", (short,))
        conn.commit()
    if delete_files:
        d = get_feed_download_dir(short)
        if os.path.isdir(d):
            removed = 0
            for root, dirs, files in os.walk(d):
                for fn in files:
                    p = os.path.join(root, fn)
                    with contextlib.suppress(Exception):
                        os.unlink(p)
                        removed += 1
            print(
                f"Cleared download records for '{short}' and deleted {removed} files."
            )
            return
    print(f"Cleared download records for '{short}'.")


def _episodes_for_download(
    conn: sqlite3.Connection,
    feed: str,
    latest: Optional[int],
    all_flag: bool,
    ids: Optional[List[int]],
    since: Optional[str],
) -> List[sqlite3.Row]:
    base = (
        "SELECT e.* FROM episodes e LEFT JOIN downloads d ON d.episode_id = e.id AND d.status = 'completed' WHERE e.feed = ? AND d.id IS NULL"
    )
    args = [feed]
    if ids:
        placeholders = ",".join("?" for _ in ids)
        base += f" AND e.id IN ({placeholders})"
        args.extend(ids)
    if since:
        base += " AND (e.pub_date IS NOT NULL AND e.pub_date >= ?)"
        args.append(since)
    base += " ORDER BY COALESCE(e.pub_date, '') DESC, e.id DESC"
    if latest and not all_flag and not ids:
        base += f" LIMIT {int(latest)}"
    return list(conn.execute(base, args))


def _derive_filename(title: Optional[str], enclosure_url: Optional[str]) -> str:
    base = slugify(title or "episode")
    ext = ""
    if enclosure_url:
        path = urllib.parse.urlparse(enclosure_url).path
        ext = os.path.splitext(path)[1]
        if not ext and "." in path:
            ext = "." + path.rsplit(".", 1)[-1]
        if ext and len(ext) > 6:
            ext = ""
    if ext.lower() not in (".mp3", ".m4a", ".aac", ".ogg", ".wav", ".flac", ".opus"):
        ext = ext if ext else ".mp3"
    return base + ext


def _download_one(
    conn: sqlite3.Connection, feed: str, row: sqlite3.Row, out_dir: str
) -> Tuple[bool, Optional[str]]:
    eid = row["id"]
    title = row["title"]
    enc = row["enclosure_url"]
    if not enc:
        eprint(f"  [id={eid}] Skipped: no enclosure URL in RSS item")
        return False, None
    fname = _derive_filename(title, enc)
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, fname)
    existing = conn.execute(
        "SELECT 1 FROM downloads WHERE episode_id = ? AND status = 'completed'",
        (eid,),
    ).fetchone()
    if existing and os.path.exists(out_path):
        print(f"  [id={eid}] Already downloaded: {fname}")
        return True, out_path
    print(f"  [id={eid}] Downloading: {title or '(no title)'}")
    try:
        written, clen = http_stream_to_file(enc, out_path)
        conn.execute(
            "INSERT INTO downloads(episode_id, feed, file_path, status, size_bytes, downloaded_at) VALUES (?, ?, ?, ?, ?, ?)",
            (eid, feed, out_path, "completed", int(written), now_iso()),
        )
        conn.commit()
        print(f"    Saved: {fname} ({human_size(written)})")
        return True, out_path
    except Exception as e:
        eprint(f"    Failed: {e}")
        with contextlib.suppress(Exception):
            conn.execute(
                "INSERT INTO downloads(episode_id, feed, file_path, status, size_bytes, downloaded_at) VALUES (?, ?, ?, ?, ?, ?)",
                (eid, feed, out_path, "failed", None, now_iso()),
            )
            conn.commit()
        with contextlib.suppress(Exception):
            if os.path.exists(out_path):
                os.unlink(out_path)
        return False, None


def cmd_download(args):
    feed = args.feed
    latest = args.latest
    all_flag = args.all
    ids = [int(x) for x in args.ids.split(",")] if args.ids else None
    since = args.since
    if not feed:
        raise SystemExit("Use --feed FEED.")
    with connect_db() as conn:
        require_feed_exists(conn, feed)
        rows = _episodes_for_download(conn, feed, latest, all_flag, ids, since)
        if not rows:
            print("Nothing to download.")
            return
        out_dir = get_feed_download_dir(feed)
        ok = 0
        for r in rows:
            success, _ = _download_one(conn, feed, r, out_dir)
            ok += int(success)
        print(f"Downloaded {ok}/{len(rows)} episodes.")


def cmd_download_title(args):
    feed = args.feed
    title_q = args.title.strip().lower()
    if not (feed and title_q):
        raise SystemExit("Use --feed FEED and --title 'text'.")
    with connect_db() as conn:
        require_feed_exists(conn, feed)
        row = conn.execute(
            "SELECT * FROM episodes WHERE feed = ? AND LOWER(title) LIKE ? ORDER BY COALESCE(pub_date,'') DESC, id DESC LIMIT 1",
            (feed, f"%{title_q}%"),
        ).fetchone()
        if not row:
            print("No matching episode found.")
            return
        out_dir = get_feed_download_dir(feed)
        _download_one(conn, feed, row, out_dir)


def _print_episode_list(rows: List[sqlite3.Row], show_feed: bool = False):
    if not rows:
        print("No episodes.")
        return
    for r in rows:
        pd = r["pub_date"] or ""
        title = r["title"] or "(no title)"
        eid = r["id"]
        if show_feed:
            print(f"[{r['feed']}] id={eid}  {pd}  {title}")
        else:
            print(f"id={eid}  {pd}  {title}")


def cmd_search(args):
    feed = args.feed
    query = (args.query or "").strip().lower()
    if not (feed and query):
        raise SystemExit("Use --feed FEED and --query 'text'.")
    with connect_db() as conn:
        require_feed_exists(conn, feed)
        rows = list(
            conn.execute(
                "SELECT * FROM episodes WHERE feed = ? AND (LOWER(title) LIKE ? OR LOWER(description) LIKE ?) ORDER BY COALESCE(pub_date,'') DESC, id DESC",
                (feed, f"%{query}%", f"%{query}%"),
            )
        )
    _print_episode_list(rows)


def cmd_search_all(args):
    query = (args.query or "").strip().lower()
    if not query:
        raise SystemExit("Use --query 'text'.")
    with connect_db() as conn:
        rows = list(
            conn.execute(
                "SELECT * FROM episodes WHERE (LOWER(title) LIKE ? OR LOWER(description) LIKE ?) ORDER BY COALESCE(pub_date,'') DESC, id DESC",
                (f"%{query}%", f"%{query}%"),
            )
        )
    _print_episode_list(rows, show_feed=True)


def cmd_queue_add(args):
    feed = args.feed
    eid = args.episode_id
    title_q = (args.title or "").strip().lower()
    if not feed:
        raise SystemExit("Use --feed FEED.")
    if not (eid or title_q):
        raise SystemExit("Provide --episode-id ID or --title 'text'.")
    with connect_db() as conn:
        require_feed_exists(conn, feed)
        if eid:
            row = conn.execute(
                "SELECT * FROM episodes WHERE id = ? AND feed = ?", (eid, feed)
            ).fetchone()
        else:
            row = conn.execute(
                "SELECT * FROM episodes WHERE feed = ? AND LOWER(title) LIKE ? ORDER BY COALESCE(pub_date,'') DESC, id DESC LIMIT 1",
                (feed, f"%{title_q}%"),
            ).fetchone()
        if not row:
            raise SystemExit("Episode not found.")
        existing = conn.execute(
            "SELECT 1 FROM queue WHERE episode_id = ?", (row["id"],)
        ).fetchone()
        if existing:
            print(f"Already in queue: id={row['id']}  {row['title']}")
            return
        conn.execute(
            "INSERT INTO queue(episode_id, feed, added_at) VALUES (?, ?, ?)",
            (row["id"], feed, now_iso()),
        )
        conn.commit()
        print(f"Queued: id={row['id']}  {row['title']}")


def cmd_queue_list(args):
    with connect_db() as conn:
        rows = list(
            conn.execute(
                """
        SELECT q.id as qid, e.id as eid, q.feed as feed, e.title as title, e.pub_date as pub_date
        FROM queue q JOIN episodes e ON e.id = q.episode_id
        ORDER BY q.added_at ASC
        """
            )
        )
    if not rows:
        print("Queue is empty.")
        return
    for r in rows:
        print(
            f"queue_id={r['qid']}  [{r['feed']}] id={r['eid']}  {r['pub_date'] or ''}  {r['title'] or '(no title)'}"
        )


def cmd_queue_remove(args):
    qid = args.queue_id
    eid = args.episode_id
    with connect_db() as conn:
        if qid:
            conn.execute("DELETE FROM queue WHERE id = ?", (qid,))
        elif eid:
            conn.execute("DELETE FROM queue WHERE episode_id = ?", (eid,))
        else:
            raise SystemExit("Use --queue-id QID or --episode-id ID.")
        conn.commit()
    print("Removed from queue.")


def cmd_queue_reset(args):
    with connect_db() as conn:
        conn.execute("DELETE FROM queue")
        conn.commit()
    print("Queue cleared.")


def cmd_queue_download(args):
    with connect_db() as conn:
        rows = list(
            conn.execute(
                """
        SELECT q.id as qid, e.* FROM queue q JOIN episodes e ON e.id = q.episode_id ORDER BY q.added_at ASC
        """
            )
        )
        if not rows:
            print("Queue is empty.")
            return
        per_feed_dirs: Dict[str, str] = {}
        ok = 0
        for r in rows:
            feed = r["feed"]
            out_dir = per_feed_dirs.get(feed)
            if not out_dir:
                out_dir = get_feed_download_dir(feed)
                os.makedirs(out_dir, exist_ok=True)
                per_feed_dirs[feed] = out_dir
            success, _ = _download_one(conn, feed, r, out_dir)
            ok += int(success)
            # FIX b0001: use queue row id (qid) when deleting, not episode id
            conn.execute("DELETE FROM queue WHERE id = ?", (r["qid"],))
            conn.commit()
        print(f"Downloaded {ok}/{len(rows)} queued episodes.")


def cmd_playlist(args):
    args.playlist_func(args)


def playlist_create(args):
    name = args.name
    with connect_db() as conn:
        conn.execute(
            "INSERT OR IGNORE INTO playlists(name, created_at) VALUES (?, ?)",
            (name, now_iso()),
        )
        conn.commit()
    print(f"Playlist created: {name}")


def playlist_list(args):
    with connect_db() as conn:
        rows = list(
            conn.execute(
                "SELECT name, created_at FROM playlists ORDER BY name ASC"
            )
        )
    if not rows:
        print("No playlists.")
        return
    for r in rows:
        print(f"{r['name']}  (created {r['created_at']})")


def playlist_add(args):
    name = args.name
    eid = args.episode_id
    if not eid:
        raise SystemExit("Provide --episode-id ID.")
    with connect_db() as conn:
        pl = conn.execute(
            "SELECT 1 FROM playlists WHERE name = ?", (name,)
        ).fetchone()
        if not pl:
            raise SystemExit(f"Playlist '{name}' not found. Create it first.")
        ep = conn.execute(
            "SELECT id FROM episodes WHERE id = ?", (eid,)
        ).fetchone()
        if not ep:
            raise SystemExit("Episode not found.")
        pos_row = conn.execute(
            "SELECT COALESCE(MAX(position), 0) AS p FROM playlist_items WHERE playlist_name = ?",
            (name,),
        ).fetchone()
        pos = int(pos_row["p"]) + 1
        conn.execute(
            "INSERT OR REPLACE INTO playlist_items(playlist_name, episode_id, position, added_at) VALUES (?, ?, ?, ?)",
            (name, eid, pos, now_iso()),
        )
        conn.commit()
    print(f"Added episode {eid} to playlist '{name}' at position {pos}.")


def playlist_remove(args):
    name = args.name
    eid = args.episode_id
    with connect_db() as conn:
        conn.execute(
            "DELETE FROM playlist_items WHERE playlist_name = ? AND episode_id = ?",
            (name, eid),
        )
        conn.commit()
    print(f"Removed episode {eid} from playlist '{name}'.")


def playlist_show(args):
    name = args.name
    with connect_db() as conn:
        rows = list(
            conn.execute(
                """
        SELECT pi.position as pos, e.id as eid, e.title as title, e.pub_date as pub_date, e.feed as feed
        FROM playlist_items pi JOIN episodes e ON e.id = pi.episode_id
        WHERE pi.playlist_name = ?
        ORDER BY pi.position ASC
        """,
                (name,),
            )
        )
    if not rows:
        print(f"Playlist '{name}' is empty or does not exist.")
        return
    for r in rows:
        print(
            f"{r['pos']:>3d}. [{r['feed']}] id={r['eid']}  {r['pub_date'] or ''}  {r['title'] or '(no title)'}"
        )


def playlist_export(args):
    name = args.name
    out = os.path.expanduser(args.out)
    with connect_db() as conn:
        rows = list(
            conn.execute(
                """
        SELECT d.file_path FROM playlist_items pi
        JOIN episodes e ON e.id = pi.episode_id
        JOIN downloads d ON d.episode_id = e.id AND d.status = 'completed'
        WHERE pi.playlist_name = ?
        ORDER BY pi.position ASC
        """,
                (name,),
            )
        )
    if not rows:
        raise SystemExit("Nothing to export. Ensure episodes are downloaded.")
    os.makedirs(os.path.dirname(os.path.abspath(out)), exist_ok=True)
    with open(out, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for r in rows:
            f.write(os.path.abspath(r["file_path"]) + "\n")
    print(f"Exported playlist '{name}' to {out}")

# ---------- yt-dlp helpers ----------


def run_yt_dlp(link: str, video: bool, out_dir: str):
    """Invoke yt-dlp to download a link as audio (default) or video.
    Requires `yt-dlp` to be available on PATH. Raises on failure.
    """
    out_dir = os.path.expanduser(out_dir)
    os.makedirs(out_dir, exist_ok=True)
    # filename template: title + id to avoid collisions
    template = os.path.join(out_dir, "%(title).120s-%(id)s.%(ext)s")

    if video:
        args = [
            "yt-dlp",
            "-f",
            "bestvideo*+bestaudio/best",
            "--merge-output-format",
            "mp4",
            "-o",
            template,
            link,
        ]
    else:
        args = [
            "yt-dlp",
            "-x",
            "--audio-format",
            "mp3",
            "--audio-quality",
            "0",
            "-o",
            template,
            link,
        ]
    try:
        eprint("Running:", " ".join(args))
        subprocess.check_call(args)
    except FileNotFoundError:
        raise SystemExit(
            "yt-dlp not found. Install it, e.g. `pipx install yt-dlp` or your package manager."
        )
    except subprocess.CalledProcessError as e:
        raise SystemExit(f"yt-dlp failed with exit code {e.returncode}.")


def cmd_download_yt(args):
    link = args.link
    if not link:
        raise SystemExit("Provide --link URL")
    out_dir = get_setting("download_yt_dir", DEFAULT_YT_DIR)
    run_yt_dlp(link=link, video=args.video, out_dir=out_dir)
    print(
        f"Saved to {os.path.abspath(os.path.expanduser(out_dir))} (mode: {'video' if args.video else 'audio-only'})"
    )


# ---------- Argument Parser ----------


def pod_cmd_set(args):
    made_change = False

    if args.download_dir:
        set_setting("download_dir", args.download_dir)
        made_change = True

    if args.download_yt_dir:
        set_setting("download_yt_dir", args.download_yt_dir)
        made_change = True

    if args.change_feed_dir:
        if not args.feed_dir_path:
            raise SystemExit("Use --dir PATH with --change-feed-dir FEED")
        set_feed_download_dir(args.change_feed_dir, args.feed_dir_path)
        made_change = True
        print(f"Set custom dir for feed '{args.change_feed_dir}' → {os.path.abspath(os.path.expanduser(args.feed_dir_path))}")

    if args.unset_feed_dir:
        unset_feed_download_dir(args.unset_feed_dir)
        made_change = True
        print(f"Removed custom dir override for feed '{args.unset_feed_dir}'")

    if args.show or not made_change:
        base = get_base_downloads_dir()
        yt = get_setting("download_yt_dir", DEFAULT_YT_DIR)
        print("Current settings:")
        print(f"  Base download dir: {os.path.abspath(base)}")
        print(f"  YT download dir:   {os.path.abspath(os.path.expanduser(yt))}")
        overrides = list_feed_dir_overrides()
        if overrides:
            print("  Per-feed overrides:")
            for feed, path in sorted(overrides.items()):
                print(f"    {feed}: {os.path.abspath(path)}")
        else:
            print("  Per-feed overrides: (none)")

    ensure_dirs()
    if made_change and not args.show:
        print("Settings updated.")


def build_pod_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog=POD_APP_NAME,
        description=DESC,
    )

    sub = parser.add_subparsers(dest="cmd", required=True)

    # init
    p = sub.add_parser("init", help="Initialize config directory and PodFile")
    p.set_defaults(func=cmd_init)

    # list-feeds
    p = sub.add_parser("list-feeds", help="List all configured podcast feeds (RSS only)")
    p.set_defaults(func=cmd_list_feeds)

    # add-feed
    p = sub.add_parser("add-feed", help="Add a new podcast feed (RSS URL)")
    p.add_argument("short_name", help="Short name to refer to this feed")
    p.add_argument("url", help="RSS URL (Atom will be rejected on refresh)")
    p.set_defaults(func=cmd_add_feed)

    # remove-feed
    p = sub.add_parser("remove-feed", help="Remove a podcast feed")
    p.add_argument("short_name", help="Short name of the feed to remove")
    p.add_argument(
        "--purge-episodes",
        action="store_true",
        help="Also delete manifest entries for this feed",
    )
    p.add_argument(
        "--delete-files",
        action="store_true",
        help="Also delete downloaded files on disk",
    )
    p.set_defaults(func=cmd_remove_feed)

    # rename-feed
    p = sub.add_parser("rename-feed", help="Rename a podcast feed's short name")
    p.add_argument("old_short", help="Existing short name")
    p.add_argument("new_short", help="New short name")
    p.set_defaults(func=cmd_rename_feed)

    # refresh
    p = sub.add_parser(
        "refresh", help="Refresh episode list for a podcast (RSS validation enforced)"
    )
    g = p.add_mutually_exclusive_group(required=True)
    g.add_argument("--feed", help="Short name of the feed to refresh")
    g.add_argument("--all", action="store_true", help="Refresh all feeds")
    p.set_defaults(func=cmd_refresh)

    # info
    p = sub.add_parser("info", help="Show detailed info about a podcast")
    p.add_argument("short_name", help="Feed short name")
    p.set_defaults(func=cmd_info)

    # set-summary
    p = sub.add_parser("set-summary", help="Set a custom podcast summary")
    p.add_argument("short_name", help="Feed short name")
    p.add_argument("summary", help="Summary text")
    p.set_defaults(func=cmd_set_summary)

    # clean
    p = sub.add_parser(
        "clean", help="Clear all download records for a podcast"
    )
    p.add_argument("short_name", help="Feed short name")
    p.add_argument(
        "--delete-files",
        action="store_true",
        help="Also remove downloaded files from disk",
    )
    p.set_defaults(func=cmd_clean)

    # download
    p = sub.add_parser("download", help="Download episodes from a feed")
    p.add_argument("--feed", required=True, help="Feed short name")
    p.add_argument(
        "-L",
        "--latest",
        type=int,
        default=1,
        help="Download latest N not-yet-downloaded episodes (default: 1)",
    )
    p.add_argument("--all", action="store_true", help="Download all not-yet-downloaded episodes")
    p.add_argument("--ids", help="Comma-separated episode IDs to download")
    p.add_argument(
        "--since", help="Only episodes with pub_date >= ISO date (e.g., 2024-01-01)"
    )
    p.set_defaults(func=cmd_download)

    # download-title
    p = sub.add_parser("download-title", help="Download a specific episode by title")
    p.add_argument("--feed", required=True, help="Feed short name")
    p.add_argument("--title", required=True, help="Case-insensitive substring to match")
    p.set_defaults(func=cmd_download_title)

    # search
    p = sub.add_parser("search", help="Search for episodes in a single podcast")
    p.add_argument("--feed", required=True, help="Feed short name")
    p.add_argument("--query", required=True, help="Text to search in title/description")
    p.set_defaults(func=cmd_search)

    # search-all
    p = sub.add_parser(
        "search-all", help="Search for text across all episodes of all podcasts"
    )
    p.add_argument("--query", required=True, help="Text to search in title/description")
    p.set_defaults(func=cmd_search_all)

    # queue-add
    p = sub.add_parser("queue-add", help="Add an episode to the download queue")
    p.add_argument("--feed", required=True, help="Feed short name")
    p.add_argument("--episode-id", type=int, help="Episode ID")
    p.add_argument("--title", help="Case-insensitive substring to match")
    p.set_defaults(func=cmd_queue_add)

    # queue-list
    p = sub.add_parser("queue-list", help="List all episodes in the download queue")
    p.set_defaults(func=cmd_queue_list)

    # queue-download
    p = sub.add_parser(
        "queue-download", help="Download all episodes in the queue and clear it"
    )
    p.set_defaults(func=cmd_queue_download)

    # queue-remove
    p = sub.add_parser("queue-remove", help="Remove an episode from the queue")
    p.add_argument("--queue-id", type=int, help="Queue entry id")
    p.add_argument("--episode-id", type=int, help="Episode id")
    p.set_defaults(func=cmd_queue_remove)

    # queue-reset
    p = sub.add_parser("queue-reset", help="Clear the download queue completely")
    p.set_defaults(func=cmd_queue_reset)

    # playlist
    p = sub.add_parser("playlist", help="Manage playlists of downloaded episodes")
    pl_sub = p.add_subparsers(dest="playlist_cmd", required=True)

    p1 = pl_sub.add_parser("create", help="Create a new playlist")
    p1.add_argument("name")
    p1.set_defaults(playlist_func=playlist_create)

    p2 = pl_sub.add_parser("list", help="List playlists")
    p2.set_defaults(playlist_func=playlist_list)

    p3 = pl_sub.add_parser("add", help="Add an episode to a playlist")
    p3.add_argument("name")
    p3.add_argument("--episode-id", type=int, required=True)
    p3.set_defaults(playlist_func=playlist_add)

    p4 = pl_sub.add_parser("remove", help="Remove an episode from a playlist")
    p4.add_argument("name")
    p4.add_argument("--episode-id", type=int, required=True)
    p4.set_defaults(playlist_func=playlist_remove)

    p5 = pl_sub.add_parser("show", help="Show playlist items")
    p5.add_argument("name")
    p5.set_defaults(playlist_func=playlist_show)

    p6 = pl_sub.add_parser("export", help="Export playlist as M3U")
    p6.add_argument("name")
    p6.add_argument("--out", required=True, help="Output .m3u path")
    p6.set_defaults(playlist_func=playlist_export)

    # --- settings ---
    p = sub.add_parser("set", help="Set configuration values")
    p.add_argument(
        "--download-dir",
        help="Base directory for podcast audio downloads (default layout base/<feed>)",
    )
    p.add_argument(
        "--download-yt-dir",
        help="Directory for yt-dlp downloads (audio/video)",
    )
    p.add_argument(
        "--change-feed-dir",
        dest="change_feed_dir",
        help="Feed short name to set a custom download directory for",
    )
    p.add_argument(
        "--dir",
        dest="feed_dir_path",
        help="Directory to use with --change-feed-dir",
    )
    p.add_argument(
        "--unset-feed-dir",
        dest="unset_feed_dir",
        help="Feed short name to remove custom directory override",
    )
    p.add_argument(
        "--show",
        action="store_true",
        help="Show current directory settings and overrides",
    )
    p.set_defaults(func=pod_cmd_set)

    # --- yt-dlp download ---
    p = sub.add_parser(
        "download-yt",
        help="Download a link with yt-dlp (default audio-only).",
    )
    p.add_argument("--link", required=True, help="YouTube/yt-dlp-supported URL")
    p.add_argument(
        "--video",
        action="store_true",
        help="Download full video instead of extracting audio",
    )
    p.set_defaults(func=cmd_download_yt)

    return parser

# ---------- Main ----------


def pox_main(argv=None):
    parser = build_pod_parser()
    args = parser.parse_args(argv)
    try:
        args.func(args)
    except KeyboardInterrupt:
        eprint("Interrupted.")
        sys.exit(130)
    except SystemExit:
        raise
    except Exception as e:
        eprint(f"Error: {e}")
        if os.environ.get("POX_DEBUG"):
            traceback.print_exc()
        sys.exit(1)


# (pod module entry disabled in combined build)


# ---- End: podcast manager ----


# ---- Combined dispatcher ----
def _combined_main(argv=None):
    import sys as _sys
    if argv is None:
        argv = _sys.argv[1:]
    # Route to 'pod' subsystem if first arg is 'pod'/'pox'/'podcast'
    if argv and argv[0] in ("pod", "pox", "podcast", "media"):
        # Call podcast manager main with remaining args
        try:
            return pox_main(argv[1:])
        except SystemExit as e:
            raise
    else:
        # Default to clipex behavior
        try:
            return clipex_main(argv)
        except SystemExit as e:
            raise

if __name__ == "__main__":
    _combined_main()
